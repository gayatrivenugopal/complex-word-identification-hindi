{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/Colab/CleanedData/'\n",
        "stoplemma_path = '/content/drive/MyDrive/Colab/CleanedData/stoplemmas.txt'"
      ],
      "metadata": {
        "id": "VfeM7cK5NW37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n",
        "!pip install pyiwn\n",
        "!pip install -q fasttext"
      ],
      "metadata": {
        "id": "cnRQYjjcV9AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('hi')\n",
        "nlp = stanza.Pipeline('hi')\n"
      ],
      "metadata": {
        "id": "AREQEHR6j9Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Source: https://stackoverflow.com/questions/44474085/how-to-separate-a-only-hindi-script-from-a-file-containing-a-mixture-of-hindi-e\n",
        "def is_hindi(token):\n",
        "  for character in token:\n",
        "    if character is None or character.strip() == '':\n",
        "        return 0\n",
        "    maxchar = max(character)\n",
        "    if u'\\u0900' <= maxchar <= u'\\u097f':\n",
        "        pass\n",
        "    else:\n",
        "      return 0\n",
        "  return 1"
      ],
      "metadata": {
        "id": "bEIosvkqLCyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def update_feature_values(dataset_path, feature_name, feature_values_path, common_feature_name):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "  dataset_path (str): the path to the dataset to be updated\n",
        "  feature_name (str): the feature whose values are to be updated\n",
        "  feature_values_path (str): the file containing the new values of the feature\n",
        "  common_feature_name (str): the feature that is common between the dataset and \n",
        "  the file in the feature_values_path, that can be used to select the required row\n",
        "  \"\"\"\n",
        "  shutil.copyfile(dataset_path, \"./\")\n",
        "  feature_values = pd.read_csv(feature_values_path)\n",
        "  df = pd.read_csv(dataset_path)\n",
        "  for row in df.iterrows():\n",
        "    if(row[common_feature_name] in feature_values[common_feature_name].values()):\n",
        "      row.loc[common_feature_name, feature_name] = feature_values[common_feature_name][feature_name]\n",
        "  df.to_csv(dataset_path)\n",
        "'''"
      ],
      "metadata": {
        "id": "x6hWQD3kl4FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Lemmas"
      ],
      "metadata": {
        "id": "hUZCjogt4aA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stoplemmas = list()\n",
        "with open(stoplemma_path, 'r', encoding = 'utf-8') as stoplemmas_file:\n",
        "  for line in stoplemmas_file:\n",
        "    stoplemmas.append(line.split(\",\")[0])"
      ],
      "metadata": {
        "id": "VwKl6Poc4cfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Processing"
      ],
      "metadata": {
        "id": "R_5tuesR5K5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sentence):\n",
        "  #remove punctuations\n",
        "  for symbol in string.punctuation:\n",
        "    if symbol != ' ':\n",
        "      sentence = sentence.replace(symbol, \"\")\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "x3XEWaso5Og1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch values of lexical charactertistics\n",
        "import pyiwn\n",
        "import stanza\n",
        "\n",
        "pyiwn.download()\n",
        "iwn = pyiwn.IndoWordNet()\n",
        "\n",
        "vowels_list = ['ऽ', 'ँ', 'ं', 'ः', 'ऺ', 'ऻ', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'ॅ', 'ॆ', 'े', 'ै', 'ॉ', 'ॊ', 'ो', 'ौ',\n",
        "         '्', 'ॎ', 'ॏ', '॑', '॒', '॓', '॔', 'ॕ', 'ॖ', 'ॗ', 'ॢ', 'ॣ', '॰', 'ॱ', '।', '॥','अ', 'आ','इ','ई','उ','ऊ',\n",
        "         'ऋ','ए','ऐ','ओ','औ','अं','अः']\n",
        "consonants_list = ['क','ख','ग','घ','ङ','च','छ','ज','झ','ञ','ट','ठ','ड','ढ','ण','त',\n",
        "'थ','द','ध','न','प','फ','ब','भ','म','य','र','ल','व','श','ष','स','ह','क्ष','त्र','ज्ञ']\n",
        "\n",
        "def get_length(word):\n",
        "    word = word.strip()\n",
        "    return len(word)\n",
        "\n",
        "def get_root(word):\n",
        "    \"\"\"\n",
        "    Return the root form of the specified word.\n",
        "    Required argument:\n",
        "\tword (str): the word whose root form is to be retrieved\n",
        "    \"\"\"\n",
        "    word = word.strip()\n",
        "    doc = nlp(word)\n",
        "    for sentence in doc.sentences:\n",
        "        for word in sentence.words:\n",
        "            if word.lemma != None and word.lemma != '':\n",
        "                return word.lemma\n",
        "            return word\n",
        "\n",
        "def get_synsets(word):\n",
        "    word = word.strip()\n",
        "    try:\n",
        "      temp_synsets = iwn.synsets(word)\n",
        "      return temp_synsets\n",
        "    except:\n",
        "      return \"\"\n",
        "    \n",
        "\n",
        "def get_number_of_synsets(word):\n",
        "    #print(word)\n",
        "    word = word.strip()\n",
        "    return len(iwn.synsets(word))\n",
        "\n",
        "def get_synonyms_in_synset(synset):\n",
        "    synonyms = []\n",
        "    #print(\"fetching synonyms in synset\")\n",
        "    lemma_list = synset.lemmas()\n",
        "    for i in range(0, len(lemma_list)):\n",
        "        if lemma_list[i].name() not in synonyms:\n",
        "            synonyms.append(lemma_list[i].name())\n",
        "    #print(\"returning synonyms\")\n",
        "    return synonyms\n",
        "\n",
        "def get_number_of_synonyms_in_synset(synset):\n",
        "    synonyms = []\n",
        "    lemma_list = synset.lemmas()\n",
        "    for i in range(0, len(lemma_list)):\n",
        "        if lemma_list[i].name() not in synonyms:\n",
        "            synonyms.append(lemma_list[i].name())\n",
        "    return len(synonyms)\n",
        "\n",
        "def get_number_of_hypernyms(word):\n",
        "  n_hypernyms=0\n",
        "  #for v in iwn.all_synsets():\n",
        "  #  print(\"totalling number of hypernyms for \", v)\n",
        "  synsets = get_synsets(word)\n",
        "  for synset in synsets:\n",
        "      n_hypernyms += len(iwn.synset_relation(synset, pyiwn.SynsetRelations.HYPONYMY))\n",
        "  #print(\"calculated total\")\n",
        "  return n_hypernyms\n",
        "\n",
        "def get_number_of_hyponyms(word):\n",
        "  n_hyponyms=0\n",
        "  synsets = get_synsets(word)\n",
        "  for synset in synsets:\n",
        "    n_hyponyms += len(iwn.synset_relation(synset, pyiwn.SynsetRelations.HYPERNYMY))\n",
        "  return n_hyponyms\n",
        "\n",
        "\n",
        "def get_synset_id(synset):\n",
        "    return synset.synset_id()\n",
        "\n",
        "def get_synset_gloss(synset):\n",
        "    return synset.gloss()\n",
        "\n",
        "def get_word_gloss(word):\n",
        "    word = word.strip()\n",
        "    synsets = get_synsets(word)\n",
        "    gloss_list = []\n",
        "    for synset in synsets:\n",
        "        gloss_list.append(synset.gloss())\n",
        "    return gloss_list\n",
        "\n",
        "def get_synset_examples(synset):\n",
        "    return synset.examples()\n",
        "\n",
        "def get_ontology_nodes(synset):\n",
        "    #print(synset.ontology_nodes())\n",
        "    return synset.ontology_nodes()\n",
        "\n",
        "def is_person(synset):\n",
        "    \"\"\" Return true if all the nodes are person nodes. \"\"\"\n",
        "    person = True\n",
        "    ontology_list = get_ontology_nodes(synset)\n",
        "    for item in ontology_list:\n",
        "        if item.find('PRSN') == -1:\n",
        "            person = False\n",
        "    return person\n",
        "\n",
        "def is_place(synset):\n",
        "    \"\"\" Return true if all the nodes are physical place nodes. \"\"\"\n",
        "    place = True\n",
        "    ontology_list = get_ontology_nodes(synset)\n",
        "    for item in ontology_list:\n",
        "        if item.find('PHSCL') == -1:\n",
        "            place = False\n",
        "    return place\n",
        "\n",
        "def get_number_of_consonants(word):\n",
        "    word = word.strip()\n",
        "    consonants = 0\n",
        "    charList = list(word)\n",
        "    for character in charList:\n",
        "        if character in consonants_list:\n",
        "            consonants = consonants + 1\n",
        "    return consonants\n",
        "\n",
        "def get_number_of_vowels(word):\n",
        "    word = word.strip()\n",
        "    vowels = 0\n",
        "    charList = list(word)\n",
        "    for character in charList:\n",
        "        if character in vowels_list:\n",
        "            vowels = vowels + 1\n",
        "    return vowels\n",
        "\n",
        "def get_number_of_consonant_conjuncts(word):\n",
        "    word = word.strip()\n",
        "    conjuncts = 0\n",
        "    charList = list(word)\n",
        "    for character in charList:\n",
        "        if character == '्':\n",
        "            conjuncts = conjuncts + 1\n",
        "    return conjuncts\n",
        "\n",
        "def get_syllable_count(word):\n",
        "    syllables = 0\n",
        "    consonants = 1\n",
        "    consonant_flag = 0\n",
        "    charList = list(word)\n",
        "    prev = -1\n",
        "    index = 1\n",
        "    syllables = 1\n",
        "    #print(charList)\n",
        "    #find the second consonant\n",
        "    for i in range(1, len(charList)):\n",
        "        character = charList[i]\n",
        "        #print(character)\n",
        "        if character in consonants_list:\n",
        "            consonants = consonants + 1\n",
        "            if consonants == 2:\n",
        "                break\n",
        "        index = index + 1\n",
        "    beg = index\n",
        "    #print(\"BEG: \", beg)\n",
        "    #print(syllables)\n",
        "    for i in range(index, len(charList)):\n",
        "        character = charList[i]\n",
        "        #print(character)\n",
        "        #character_count = character_count + 1\n",
        "        #if character_count == 1:\n",
        "        #    syllables = 1\n",
        "        #elif character in Words.consonants_list:\n",
        "        #    consonants = 1\n",
        "        #    consonant_flag = 1\n",
        "        #print(\"PREV: \", prev)\n",
        "        if character in consonants_list and syllables > 0 and i != len(charList)-1:# and i != prev + 1:\n",
        "            if (i+1 < len(charList) and charList[i+1] != '़'):\n",
        "                #print(\"Charlist i + 1: \", charList[i+1])\n",
        "                prev = i\n",
        "                syllables = syllables + 1\n",
        "                consonant_flag = consonant_flag + 1\n",
        "        elif ((character == 'य' and charList[i-1] in vowels_list) or character == 'त्र' or (character == 'र' and charList[i-1] == '्') or character == 'ज्ञ')and i == len(charList) - 1:\n",
        "            syllables = syllables + 1\n",
        "            continue\n",
        "        elif character in vowels_list and character != '़' and  character != '्' and i == len(charList)-1 and i != prev + 1:\n",
        "            if consonant_flag > 0:\n",
        "                syllables = syllables - 1\n",
        "            syllables = syllables + 1\n",
        "            prev = i\n",
        "        elif character in vowels_list and character != '़' and  character != '्' and i < len(charList)-1 and i != prev + 1 and charList[i-1] == '़':\n",
        "            syllables = syllables + 1\n",
        "            prev = i\n",
        "        if character == '्':\n",
        "            syllables = syllables - 1\n",
        "        if character == '़' and i != len(charList)-1 and i-1 != beg and i-1 == prev:\n",
        "            syllables = syllables - 1\n",
        "        if character in vowels_list and character != '़' and  character != '्' and i == len(charList)-1 and charList[i-2] in consonants_list and charList[i-1] not in vowels_list:\n",
        "            syllables = syllables - 1\n",
        "        #if character == '्' and i == len(charList)-2:\n",
        "        #    syllables = syllables + 1\n",
        "        #elif character in Words.vowels_list and syllables > 0 and consonants > 0 and consonant_flag == 1:\n",
        "         #   if character != '्':\n",
        "         #       syllables = syllables + 1\n",
        "          #      consonant_flag = 0\n",
        "        index = index + 1\n",
        "        #print(syllables)\n",
        "    return syllables\n",
        "\n",
        "def get_frequency(word):\n",
        "    lemmas_path='/content/drive/MyDrive/Colab/CleanedData/lemmas_new.txt'\n",
        "    df = pd.read_csv(lemmas_path)\n",
        "    #print(df.head())\n",
        "    #df.set_index('word', inplace = True)\n",
        "    #print(df.columns)\n",
        "    #print(df['word'] == word)\n",
        "    try:\n",
        "      #print(word, df['frequency'].to_numpy()[df['word'] == word].item())\n",
        "      return df['frequency'].to_numpy()[df['word'] == word].item()\n",
        "    except:\n",
        "      return 0"
      ],
      "metadata": {
        "id": "fAm2f7GzedGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to normalise\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "def normalise(values):\n",
        "  reshaped_list = np.array(values).reshape(-1,1)\n",
        "  scaler = preprocessing.MinMaxScaler()\n",
        "  normalised_list = scaler.fit_transform(reshaped_list)\n",
        "  return normalised_list\n"
      ],
      "metadata": {
        "id": "JaWMTKLwems8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_synonyms(word):\n",
        "    synonyms = list()\n",
        "    #print(\"fetching synsets\")\n",
        "    synsets = get_synsets(word)\n",
        "    for synset in synsets:\n",
        "      synonyms.extend(get_synonyms_in_synset(synset))\n",
        "    #print(\"returning all synonyms\")\n",
        "    return synonyms\n",
        "\n",
        "def get_freq_lexical_properties(word):\n",
        "    props = dict()\n",
        "    props['word'] = word\n",
        "    props['length'] = get_length(word)\n",
        "    props['n_synsets'] = get_number_of_synsets(word)\n",
        "    props['n_synonyms'] = len(get_all_synonyms(word))\n",
        "    #print(\"fetching n_consonants\")\n",
        "    props['n_consonants'] = get_number_of_consonants(word)\n",
        "    #print(\"fetching n_vowels\")\n",
        "    props['n_vowels'] = get_number_of_vowels(word)\n",
        "    #print(\"fetching n_hypernyms\")\n",
        "    props['n_hypernyms'] = get_number_of_hypernyms(word)\n",
        "    #print(\"fetching n_hyponyms\")\n",
        "    props['n_hyponyms'] = get_number_of_hyponyms(word)\n",
        "    #print(\"fetching n_consonant conjuncts\")\n",
        "    props['n_consonantconjuncts'] = get_number_of_consonant_conjuncts(word)\n",
        "    #print(\"fetching n_syllables\")\n",
        "    props['n_syllables'] = get_syllable_count(word)\n",
        "    #print(\"fetching frequency\")\n",
        "    props['frequency'] = get_frequency(word)\n",
        "    #print(\"returning raw values of all features\")\n",
        "    return props\n",
        "\n",
        "def create_wordgroup(word):\n",
        "    reduce_synonyms = 0\n",
        "    wordgroup = list()\n",
        "    word = get_root(word)\n",
        "    #print(word)\n",
        "    synonyms = set()\n",
        "    synonyms.update(word)\n",
        "    #print(\"fetching all synonyms\")\n",
        "    synonyms.update(get_all_synonyms(word))\n",
        "    for token in synonyms:\n",
        "      if token in vowels_list or token in consonants_list:\n",
        "        reduce_synonyms += 1\n",
        "        continue\n",
        "      props = get_freq_lexical_properties(token)\n",
        "      props['n_synonyms'] -= reduce_synonyms\n",
        "      # props contains raw un-normalised values\n",
        "      wordgroup.append(props)\n",
        "    #[{'word':word, 'length':length, ....}, {'word':word, 'length':length, ....}, ...]\n",
        "    return wordgroup\n",
        "\n",
        "def normalise_wordgroup(wordgroup):\n",
        "    import pandas as pd\n",
        "    props = ['word', 'length', 'n_synsets', 'n_synonyms', 'n_consonants', 'n_vowels', \n",
        "             'n_hypernyms', 'n_hyponyms', 'n_consonantconjuncts', 'n_syllables', 'frequency']\n",
        "    feature_dict = dict()\n",
        "    #print(pd.DataFrame.from_dict(wordgroup))\n",
        "    for word_dict in wordgroup:\n",
        "      for key, value in word_dict.items():\n",
        "        key = key.strip()\n",
        "        #print(\"key: \", key)\n",
        "        if key not in feature_dict.keys():\n",
        "          feature_dict[key] = list()\n",
        "        feature_dict[key].append(value)\n",
        "    #print(\"feature dict: \", pd.DataFrame.from_dict(feature_dict))\n",
        "    normalised_feature_dict = dict()\n",
        "    for key, value in feature_dict.items():\n",
        "      if key != 'word':\n",
        "        normalised_feature_dict[key] = normalise(feature_dict[key])\n",
        "        #if key == 'frequency':\n",
        "        #   print('word: ', feature_dict['word'], ' old: ', feature_dict[key], ' new: ', normalised_feature_dict[key])\n",
        "      else:\n",
        "         normalised_feature_dict[key] = feature_dict[key]\n",
        "         #print(\"word: \", feature_dict[key])\n",
        "    #print(normalised_feature_dict)\n",
        "    for key, value_list in normalised_feature_dict.items():\n",
        "      if key != 'word':\n",
        "        #normalised_feature_dict[key] = [element for innerList in value for element in innerList]\n",
        "        normalised_feature_dict[key] = list()\n",
        "        for value in value_list:\n",
        "          normalised_feature_dict[key].append(value[0])\n",
        "      #print(\"value: \", value)\n",
        "    #print(\"NORMALISED: \", normalised_feature_dict)\n",
        "    #df = pd.DataFrame.from_dict(normalised_feature_dict)\n",
        "    df = pd.DataFrame.from_dict(normalised_feature_dict)\n",
        "    #df = df.reset_index()\n",
        "    #print(df)\n",
        "    return df\n",
        "\n",
        "#normalise_wordgroup(create_wordgroup('बोलता'))\n"
      ],
      "metadata": {
        "id": "q50K9H0zuHdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to fetch embeddings\n",
        "#from google.colab import drive\n",
        "import fasttext\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file = '/content/drive/MyDrive/Colab/AI4Bharat/indicnlp.v1.hi.bin'\n",
        "\n",
        "model = fasttext.load_model(file)\n",
        "\n",
        "def vectorise(key):\n",
        "    try:\n",
        "        return model.get_word_vector(key)\n",
        "    except:\n",
        "\t      return [0]*300 #dimensions"
      ],
      "metadata": {
        "id": "zTMjgQdyekqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict"
      ],
      "metadata": {
        "id": "pkJvj1AFG4MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "stoplemmas = list()\n",
        "\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "\n",
        "def get_lemma(word):\n",
        "    \"\"\"\n",
        "    Return the root form of the specified word.\n",
        "    Args:\n",
        "\t    word (str): the word whose root form is to be retrieved\n",
        "    \"\"\"\n",
        "    word = word.strip()\n",
        "    doc = nlp(word)\n",
        "    for sentence in doc.sentences:\n",
        "        for word in sentence.words:\n",
        "            if word.lemma != None and word.lemma != '':\n",
        "                return word.lemma\n",
        "            return word\n",
        "\n",
        "def create_record(word_lemma):\n",
        "  vector = vectorise(word_lemma)\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  #fasttext vector of the word\n",
        "  df['vector'] = vector\n",
        "  X_vector = pd.DataFrame(df['vector'].values.tolist())\n",
        "  scaler = preprocessing.MinMaxScaler().fit(X_vector)\n",
        "  X_vector =  pd.DataFrame(scaler.transform(X_vector))\n",
        "  df.index = X_vector.index\n",
        "  X_vector = X_vector.transpose()\n",
        "\n",
        "  #normalised wordgroup\n",
        "  columns = ['length', 'n_synsets', 'n_synonyms', 'n_consonants', 'n_vowels', 'n_hypernyms', 'n_hyponyms', 'n_consonantconjuncts', 'n_syllables', 'frequency']\n",
        "  wordgroup = create_wordgroup(word_lemma)\n",
        "  normalised_wordgroup = normalise_wordgroup(wordgroup)\n",
        "\n",
        "  #append lexical feature values to the record with embeddings\n",
        "  for column in columns:\n",
        "    X_vector[column] = normalised_wordgroup.loc[normalised_wordgroup['word'] == word_lemma][column].values[0]\n",
        "  return X_vector\n",
        "\n",
        "def predict(path, word_df):\n",
        "  #load the model\n",
        "  model = pickle.load(open(path + 'model', 'rb'))\n",
        "  #print(word_df.columns)\n",
        "  pred = model.predict(word_df)[0]\n",
        "  prob = model.predict_proba(word_df)[0]\n",
        "  return {'prediction':pred, 'prob':prob}\n",
        "  #model.transform(word_df)\n",
        "\n",
        "\n",
        "def process_input(sentence):\n",
        "  global entry\n",
        "  global stoplemmas\n",
        "  predictions = dict()\n",
        "\n",
        "  sentence = preprocess(sentence)\n",
        "   \n",
        "  for word in sentence.split(\" \"):\n",
        "    if word not in predictions.keys():\n",
        "      if word != \"\\n\" and word != \"\\r\\n\" and word.strip() != \"\" :\n",
        "        #ignore numbers\n",
        "        if not any(chr.isdigit() for chr in word):\n",
        "          #fetch the lemma\n",
        "          word_lemma = get_lemma(word)\n",
        "          #process the lemma if it is not in stop lemmas\n",
        "          if word_lemma not in stoplemmas:\n",
        "            #check if word exists in the wordnet\n",
        "            if get_synsets(word_lemma) == \"\":\n",
        "              predictions[word] = 0\n",
        "            else:\n",
        "              record = create_record(word_lemma)\n",
        "              result = predict(path, record)\n",
        "              #print(result)\n",
        "              predictions[word] = dict()\n",
        "              predictions[word]['prediction'] = result['prediction']\n",
        "              predictions[word]['prob'] = result['prob']\n",
        "  print(predictions)\n",
        "\n",
        "process_input(input(\"Enter a sentence\"))"
      ],
      "metadata": {
        "id": "rNTBQZxZ1KCH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}